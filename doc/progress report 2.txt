                                                         G_SUITE METRICS

Based on the complexity of the G-suite metrics data set, all our team members have divided the g-suite metrics
(Drive, Gmail, Meet, Calendar, Accounts) and each of us stated working on them.

- Google Drive:(Lavanya Goluguri):
Progress Towards Goals:  My main task for this project is to work on the drive data, find its anamolies, patterns, distributions and predictions.
In the work of progress towards this task, I have done the data cleaning and the statistical analysis on the prime parameters of the drive data. To discuss about it in detail my statistical analysis is as stated below. 

I have separated the drive realted data from the original gsuite metrics, and created a new data frame that is fexible to query on parameters.cleaned that data, dropped the rows that has all nan values, replaced the remaining nan values with the mean of the columns.
As per the suggestions from the mentor I focused more on the 1day active users and 30 day active users.
Also google forms created and edited, google documents created and edited. 
As those point to how much content is being created, and how much is being actually collaborated/updated after creation.

R1 : Coming to statistics part of the drive data the size of the data is now 831 rows X 144 columns, 
the data is collected from 03/2017 to 08/2019. 
I have focused on 1day_active_users, 30day_active_users and google_forms_created, google_forms_edited. 
I did the basic statistics on these parameters like mean, variance, distributions modelling, Maximum likelihood , 
kernel density estimators on the distributions, point estimates, correlation and covariance among parameters and 
Hypothesis testing on the above stated parameters to see if the null hypothesis is considered or 
the alternate hypothesis is considered.
https://github.com/UNCG-CSE/G_Suite_Metrics/tree/Gsuite_lavanyagoluguri

R2: I have spent a considerable amount of time on these tasks. I have spent around 1 hour per day on this project, have increased to 2-3 hours as we go by the presentation day.  

R3: The Mean of the 1day_active users, 30day_active_users is 3888 and 15955.
In the Inter quartile range there are around 3300 to 4300 1day and 30day active users.
Both the 1day_active_users and 30day_active_users data is discrete so that distribution fit for both is poisson distribution. 
The estimators I used is the kernel density estimator to check my distribution modelling. 
In the point estimates , 
I have taken samples from the 1day and 30 day active users and used the Central limit theorem to plot the mean of the samples,
which is a normal distribution.
I checked the covariance and correlation of the two parameters, it is 0.45, means they are not highly correlated.
I have calculated the confidence intervals, t- critial and z- critical values.
I formed the hypothesis tha the mean of the 1day active users is significantly different from the mean of 30 day active users,
I did the one sample ttest, two sample ttest independent and the relative testing. 
The p value is 0.32, which is >0.05 means we fail to reject the null hypothesis. 

Google Meet:(Anusha Vanama)

Our mentor suggested me to mainly focus on below 6 metrics under Google Meet.

google.meet:total_call_minutes 
google.meet:num_30day_active_users
google.meet:num_meetings
google.meet:num_calls
google.meet:average_meeting_minutes
google.meet:total_meeting_minutes

As it will be too long to document the results of all metrics here, I had documented results of only 1 metric "google.meet:total_call_minutes".
But I had documented the results of above 6 metrics using markdown which can be referred in below github link
https://github.com/UNCG-CSE/G_Suite_Metrics/tree/master/src/Anusha

R1 Work done towards the tasks with links to notebooks

Task 1: Extraction of Google Meet data and statistical evaluation
Extracted Google Meet related metrics from the G suite Metrics data set and stored in a new data frame. 
Performed Initial analysis on Google Meet data. 
Task 2: Statistical evaluation of Total Call Minutes metric of Google Meet
Calculated Mean, Mode, Variance, Standard deviation, Interquartile (IQR) range and all basic statistics for total call minutes metric.
Task 3: Distribution Modeling
Plotted Total Call Minutes data using Histograms to observe what sort of distribution will fit my data.
Kernel Density Estimation is used to find probability density function (PDF) for Total Call Minutes data.
Task 4: Finding Correlation and Co-variance within the data
Our mentor is interested in the comparison of the types of devices (ios/android etc). I tried finding Correlation between total_call_minutes_ios, total_call_minutes_android, total_call_minutes
Task 5: Point Estimates
Point estimates are used to estimate the mean of total call minutes of all days. Total Call Minutes data follows exponential distribution.
Used Central Limit Theorem by taking the distribution of many sample means. Calculated confidence intervals and z-critical values.
Task 6: Hypothesis testing
Tested whether the mean of total call minutes for Google Meet in the year 2019 differs from the year 2018 
Tested whether the Mean of number of meetings in google meet is similar to the Mean of number of meetings in google meet using iOS or Android. Used Two-Sample T-test as comparison is between means of two independent data samples for all the scenarios
Below is the github link for all the above tasks
https://github.com/UNCG-CSE/G_Suite_Metrics/tree/master/src/Anusha

R2 Number of hours spent on each task.
I did not calculate exact time, but I used to spend around 1-2 hours everyday for this project.

R3 Description of the results obtained from the tasks
Results for Task 1: Extraction of Google Meet data and statistical evaluation
There are 55 unique metric names, 29434 observations under Google Meet recorded from January 14th, 2018 to August 17th, 2019.
There are 17731 observations in 2018 and 11703 in 2019.
Some of the Metric Values of Google Meet includes Number of users, Number of Minutes, Number of Meetings, Number of Calls, 
Usage of Chromebox and Usage of Chromebase. 
We have same metric names in both years and there are no new metrics added in 2019.


Results for Task 2: Statistical evaluation of Total Call Minutes metric of Google Meet
There are 543 observations under Total Call Minutes metric of Google Meet. There are 328 observations in 2018 and 215 in 2019.
The Metric Value here is Number of total call minutes which are extracted on daily basis. Metric value is 0 for 155 days.
Maximum number of total_call_minutes is 7623 minutes recorded on 2019-07-01.
Median of metric values is 782 and Mean value is 1152.29 which is higher than median as we have 
Maximum Outliers for total call minutes for few days in both 2018 and 2019. 
Eg: There are 7623 minutes on 2019-07-01. Interquartile (IQR) range value is 1861 minutes.
Variance and Standard Deviation are not the right measures of spread as the data is having maximum outlier values.
Median absolute deviation is the right measure of spread for TotalCallMinutes metric which is around 1159.4 minutes.

Results for Task 3: Distribution Modeling
Total call minutes comes under continuous data.
It follows the exponential distribution, the exponential started high and it has a long tail that trails off to the right 

Results for Task 4: Finding Correlation and Co-variance within the data
Correlation of Android and iOS: 0.08
Correlation of Android and Total: 0.23
Correlation of iOS and Total: 0.51
The closer to 0 the correlation coefficient is, the weaker the relationship between the variables. 
Here, there is weaker relationship between Android and iOS usage of google meet.
There is better relationship between Total usage and iOS usage of google meet when compared to the relationship between Total Usage and Android Usage.
Observed that the Android usage of google meet is less when compared to iOS usage of google meet.

Results for Task 5: Point Estimates
Point estimate based on a sample of 400 observations overestimated the true call minutes mean by 22.560137 minutes.
The distribution has high skewness, and the plot revealed the data is clearly not normal. The sample drawn from this data is also having same shape.
Created a sampling distribution by taking 300 samples from our call minutes data and then making 300 point estimates of the mean.
The sampling distribution appeared to be normal after applying Central Limit Theorem.
In addition, the mean of the sampling distribution approached the true mean, there is only a difference of 3 minutes.
Calculated 95% confidence for the mean point estimate and noticed that the calculated confidence interval is very close the true call minutes mean of 1148.051565
24 out of 25 samples of the 95% confidence intervals overlapped the red line marking the true mean.

Results for Task 6: Hypothesis testing
In all the cases, Null hypothesis is that the means of both groups are the same. Alternate Hypothesis is that the mean is different.
For all scenarios, p value is less than 0.05 which means that we reject the null hypothesis. 
Observation is that the Mean of total Call Minutes of Google Meet in year 2019 is different from year 2018 and The Mean of number of meetings in google meet is different from Mean of number of meetings in google meet using iOS as well as Android


Google Calendar (Jackie Cuong):

R1: My task was the google calendar analysis.  I have done all the basic data exploration for google calendar, as well as point estimates, visualizing the data with graphs, placing the data in  histograms, splitting the data into weeks, months, years, and testing my hypothesis of whether the third week of the month was significantly different in number of users for google calendar compared to the rest of the weeks.

R2:  I spent around an average of 3 hours per week on the task, some parts of the task requiring more time than others, such as distribution of the data which took at least 3 hours by itself while basic data exploration could have taken around 30 minutes.

R3:  For my results I have noticed found that the data is spread out generally evenly, and when visualized with a histogram I noticed that the distribution is bimodal, this leads me to believe that largest peak is for spring/fall classes which has a higher attendance of students, and the smaller peak is for summer/winter semesters where there is a lower rate of students attending classes.  For my hypothesis I compared the third week of the month to the average of all the weeks combined,  using a formula the p-value that came out was -1.4. which is enough to conclude that there was a significant difference in the third week of the month compared to the rest.  The basic data exploration showed the data had a very large outlier at 10,000 but the rest of the data shows trends at 500-3,600.  Deciding to forgo the outlier the mean for the data came to be 1643 users daily and a standard deviation of 685.

- Gmail:(Henry Reichard)

Our mentor suggested me to mainly focus on below 5 metrics under Gmail.

num_emails_received
num_emails_sent
num_inbound_rejected_emails
num_inbound_spam_emails
num_inbound_non_spam_emails

R1 : 
Task 1: Extraction of Google Email data and  data cleaning
Task 2: Statistical Evaluation
Task 3: Distribution Modeling
Task 4: Point Estimates
Task 5: Finding Correlation and Co-variance within the data
Task 6: Hypothesis testing
Below is the github link for all the above tasks
https://github.com/UNCG-CSE/G_Suite_Metrics/tree/master/src/Henry

R2: Time I spent on these task: A lot, usually couple hours on the weekends before work, and couple hours on Mondays and Wednesdays after 11AM has been dedicated to my task

R3: Description of the results obtained from the tasks
Results for Task 1:
The data is collected from 03-23-2015 to 08-19-2019. I have separated the gmail related data from the original dataset, and created a new data frame and csv file so I can preform data analysis on my service. I moved all unique attributes from the rows to be there own columns, so now I have a dataframe with 28 columns and 1545 rows. Each row represents a new day from
3-23-2015 through 08-19-2019. I set the row column as index and passed it through pd.datetime. Now it is easy to plot and index the data.

I cleaned the data by dropping nan value rows and dropping 0 value rows that occurs when G-Suites are down. Luckily the number of dropped rows were marginal, around 50 rows, so it did not affect the data that much. The data is relatively clean so I didn't have to do further data cleaning. 

Results for Task 2: 
Doing this task is relatively easy, df.describe will give you the basic statistics immediatly. I did this call for the overall gmail data and the 5 attributes my mentor wants
me to focus on.
Mean: emails received =280777, emails sent = 27640, inbound rejected emails = 16359, inbound spam emails = 44881, inbound non spam emails = 162224
and the mean for inbound non-spam emails are 162224.
Max: emails received = 648823, emails sent = 56768, inbound rejected emails = 144169, inbound spam emails = 127402, inbound nonspam emails = 442820
Min: emails received = 74919, emails sent = 1181, inbound rejected emails = 0, inbound spam emails = 9578, inbound nonspam emails = 50814
Variance: emails received = 1.323908e+10, emails sent = 2.538946e+08, inbound rejected emails = 3.410658e+08, inbound spam emails = 4.104442e+08, inbound nonspam emails = 3.070721e+09

Count: 1544 for all of them.
Results for Task 3:
All the attributes are discrete and the distribution that will work with the data is the poisson distribution because it shows how many  times an event likely to occur over specific time. Since I am analyzing how users are using a service in G-Suites, more specifically gmail, over time it would make sense to use Poisson Distribution.
The estimators I used is the kernel density estimator to check my distribution modelling  I ran a kstests to see if the estimator is a good fit. 
Results for Task 4:
For the point estimates, I have taken samples from emails_received and used central limit theorem to show how distributions of many sample means are normal and the sample mean is much closer to the actual mean. I have calculated the confidence level for emails received and the sample means falls within the confidence level. I have also calculated the confidence intervals for the  z- critical values on 11 other attributes as well.
Results for Task 5:
I checked the covariance and correlation of the 5 parameters: 
Emails received and emails sent correlation: .95
Emails receive and inbound non spam emails correlation: .88
Emails sent and inbound non spam emails correlation: .83
These correlations are very strong since it very close to 1, the correlations for the rest of the 5 attributes are less than .50 and thus are weak.

Results for Task 6:
I formed the hypothesis that the number of the 1day active users is smaller in December when compared to active users for the overall year.
I preformed a 1 sample t test with 1 day active users in december and mean of 1 day users throughout the year.
The p value is 1.22800432e-10, which is <0.05 which means we to reject the null hypothesis and it shows the distributions are different. 
Thus the mean values of the sample mean (dec) and the population mean (odu) are different. Since the mean for december is 14764 and the mean for odu is 17342, and the p value 
is much less than 0.025 (the probability of finding the data in the tail ends of the distrubition, 0.025 is on the left side with values smaller than the mean) it is concluded that 
month of december has less active users.
I also did hypothesis testing emails received and emails and I conducted a 2 sample t-test. pvalue returns 0 when the probability is super small, which means it much less than 0.05 
so the null hypothesis is rejected. These distributions are different. I did hypothesis testing for inbound emails and inbound spam emails and conducted a 2 sample t -test and
the p value was super small as well and the null hypothesis is rejected. Finally I did the same hypothesis test I did for 1 day active users for all 5 attributes and I gotten the same
results, which is amount of email traffic in december is smaller and significantly different compared to the rest of the year.
