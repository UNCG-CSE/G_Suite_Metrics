                                                    G-SUITE METRICS
Software Requirements are common for every one in this project
The enviroment where this project ran is listed in the utils folder in this repository. 
Link: https://github.com/UNCG-CSE/G_Suite_Metrics/tree/master/util/Gsuite%20environment

Google Drive:(Lavanya Goluguri):

Goals:
The goals for this project is to do analysis on the data that was given to us related to G Suite by the UNCG ITS, and find the predictions and the anamolies in the data.
As part of this I have choosen the google drive service where, I have performed statistical analysis, 
distribution modelling, hypothesis testing and Time series analysis on the drive data.
I have separated the drive related data from the original gsuite metrics, and created a new data frame that is fexible to query on parameters.cleaned that data, dropped the rows that has all nan values, replaced the remaining nan values with the mean of the columns.
As per the suggestions from the mentor I focused more on the 1day active users and 30 day active users.
Also google forms created and edited, google documents created and edited. 
As those point to how much content is being created, and how much is being actually collaborated/updated after creation.

So, the metrics that are mainly focused on in the drive data are:
1day_active_users
30day_active_users
Google_forms_created
Google_forms_edited
Google_documents_created
Google_documents_edited
Google_presentations_created
Google_presentations_edited
Google_Spreadsheets_created
Google_spreadsheets_edited
Google_drawings_created
Google_drawings_edited
Google_forms_created_delta
Google_documents_created_delta.
Tasks:
In oreder to achieve the final goals of the project I have followed a step by step method based on which I have carried out the tasks.

Task 1: Extraction of Google drive data and data cleaning
Task 2: Statistical evaluation of drive data on above mentioned metrics
Task 3: Distribution Modeling
Task 4: Finding Correlation and Co-variance for different metrics
Task 5: Point Estimates
Task 6: Hypothesis testing
Task 7: Machine Learning

Results obtained

Task 1 Results: Extraction of Google drive data and data cleaning:
I have extracted the drive data from the original data set, the drive data set is now of the size 831 rows X 144 columns. 
The data is collected from 03/2017 to 08/2019.
There few data values that are NAN, when the observations with the nan values are insignificant then i have dropped them, 
the obsevations with significant information the nan values are replaced with mean value of the metric.

Task 2: Statistical evaluation of drive data on above mentioned metrics:
I have performed statistical evaluation on the above mentioned metrics, I will discuss the results of a couple of metrics here.
The metrics considered are the google_forms_created and the google_forms_edited. The basic stats like min, max, standard deviation mean are calculated.
The values are as follows for google_forms_created mean is 26, standard deviation is 22.96, min is 0, max is 251.
The values are as follows for google_forms_edited mean is 65, standard deviation is 47.53, min is 0, max is 270.

Task 3 Results: Distribution Modeling
All the metrics in the drive data are discrete data, I have plotted the histograms of the metrics and tried to fit the distributions for the data.
Both the metrics google_forms_created and google_forms_edited have followed the poisson distribution.
I have used the Kernel density estimator for both th emetrics to fit the distribution model.

Task 4 Results: Finding Correlation and Co-variance for different metrics
The corelation and covariance between different metrics is calculated to see, if there is any significant trend in the data.
The orrelation and the covariance between the metrics google_forms_created and google_forms_edited is covariance is 933.16 and corelation is 0.85
From this we can interpret that when there is an increase in forms created there is also an increase in the number of forms edited.
Whereas the the corealation between the metrics 1day_active_users and 30day_active_users is 0.45, means they are not highly correlated.

Task 5 Results: Point Estimates
Point estimate are used to estimate the mean value of a metric. Calculated the the sample means of the metric by taking th esmaple size of 300.
plotted the sample means which follows a normal distribution using the Central limit theorem. The variations between the actual mean and the point estimates mean is only 0.079.
Calculated the confidence interval, z critical value and t-crtical value for the metrics.

Task 6 Results: Hypothesis testing
I formed the hypothesis tha the mean of the 1day active users is significantly different from the mean of 30 day active users,
I did the one sample ttest, two sample ttest independent and the relative testing. 
The p value is 0.32, which is >0.05 means we fail to reject the null hypothesis. 
The same has been done for the other metrics as well, I did all the tasks of a metric in comparision to other metric, Google_forms_created related results are compared to Google_forms_edited, and hypothesis is formulated as both are similar.
Google_documents_created results are compared to google_documents_edited and so on. 
I have formulated a different null hypothesis for each of this metrics like April month data for spreadsheets_created is different from its May month data or 2017 data for the google_presentations is similar to 2018 data of google_presentations.
The null hypothesis is rejected in few cases and in some cases based on the p value we failed to reject the null hypothesis.

Task 7 Results: Machine Learning
As the data is time series data, I did the time series analysis to predict the future values.
Functions from fb probphet are used to predict and forecast the data of 2020 based on the data of 2 years that is provided as input to the algorithm. Then the predictions are plotted with cross validations, there are few attributes that predicted with good accuracies and few others where predictions were not as expected.
I mainly considered mse(Mean squared error), mape(Mean absolute percentage error) as measures for error and accuracies.
There are some attributes where the mape values are not present as y value is close to 0.

Outcomes:
Their Implications: I find the prediction were following the similar trends as the input data in many cases, there are clear variations that can be identified in the beggining of the semester, semester break, end of the semesters.
The predictions are more accurate in the initial days of the forecast like the first 50 days than compared to the end of the days of the prediction. 
If we consider the the attributes 30day_active_users, the predictions are almost similar to the existing data and the mape scores for the initial days is only 4 percent, and in the end days of the prediction it is around 20-25 percent.
In the middle of semester, the error value of the mape in the semester breaks is more as, this is how mape works, as the y value decreases, mape is inflated, So, we see more error when the data is low.
I was also able to see the trends in the data for some attributes like what is the hourly usage, weekly usage, daily usage which can help in understanding how the google drive is being used by members of UNCG.


Below is the github link for all my tasks for the project
https://github.com/UNCG-CSE/G_Suite_Metrics/tree/master/src/lavanya


Google Meet:(Anusha Vanama):

Goals:
Below are the main goals of this project
Performing Analysis on the data set.
Finding Anomalies using metric values in the data set.
Prediction of future metric value based on the current data set values.

Our mentor suggested me to mainly focus on below 6 metrics under Google Meet. He is also interested in comparison among devices(iOS/android/jam board)
google.meet:total_call_minutes 
google.meet:num_30day_active_users
google.meet:num_meetings
google.meet:num_calls
google.meet:average_meeting_minutes
google.meet:total_meeting_minutes

Tasks
As it will be too long to document the results of all metrics here, I had documented results of few metrics.
But I had documented the results of above 6 metrics using markdown which can be referred in below github link
https://github.com/UNCG-CSE/G_Suite_Metrics/tree/master/src/Anusha

Task 1: Extraction of Google Meet data and statistical evaluation
Task 2: Statistical evaluation of Total Call Minutes metric of Google Meet
Task 3: Distribution Modeling
Task 4: Finding Correlation and Co-variance within the data
Task 5: Point Estimates
Task 6: Hypothesis testing
Task 7: Machine Learning

Results obtained

Results for Task 1: Extraction of Google Meet data and statistical evaluation
Extracted Google Meet related metrics from the G suite Metrics data set and stored in a new data frame. 
Performed Initial analysis on Google Meet data.
There are 55 unique metric names, 29434 observations under Google Meet recorded from January 14th, 2018 to August 17th, 2019.
There are 17731 observations in 2018 and 11703 in 2019.
Some of the Metric Values of Google Meet includes Number of users, Number of Minutes, Number of Meetings, Number of Calls, 
Usage of Chromebox and Usage of Chromebase. 
We have same metric names in both years and there are no new metrics added in 2019.


Results for Task 2: Statistical evaluation of Total Call Minutes metric of Google Meet
Calculated Mean, Mode, Variance, Standard deviation, Interquartile (IQR) range and all basic statistics for total call minutes metric.
There are 543 observations under Total Call Minutes metric of Google Meet. There are 328 observations in 2018 and 215 in 2019.
The Metric Value here is Number of total call minutes which are extracted on daily basis. Metric value is 0 for 155 days.
Maximum number of total_call_minutes is 7623 minutes recorded on 2019-07-01.
Median of metric values is 782 and Mean value is 1152.29 which is higher than median as we have 
Maximum Outliers for total call minutes for few days in both 2018 and 2019. 
Eg: There are 7623 minutes on 2019-07-01. Interquartile (IQR) range value is 1861 minutes.
Variance and Standard Deviation are not the right measures of spread as the data is having maximum outlier values.
Median absolute deviation is the right measure of spread for TotalCallMinutes metric which is around 1159.4 minutes.

Results for Task 3: Distribution Modeling
Plotted Total Call Minutes data using Histograms to observe what sort of distribution will fit my data.
Kernel Density Estimation is used to find probability density function (PDF) for Total Call Minutes data.
Total call minutes comes under continuous data.
It follows the exponential distribution, the exponential started high and it has a long tail that trails off to the right 

Results for Task 4: Finding Correlation and Co-variance within the data
Our mentor is interested in the comparison of the types of devices (ios/android etc). 
I tried finding Correlation between total_call_minutes_ios, total_call_minutes_android, total_call_minutes
Correlation of Android and iOS: 0.08
Correlation of Android and Total: 0.23
Correlation of iOS and Total: 0.51
The closer to 0 the correlation coefficient is, the weaker the relationship between the variables. 
Here, there is weaker relationship between Android and iOS usage of google meet.
There is better relationship between Total usage and iOS usage of google meet when compared to the relationship between Total Usage and Android Usage.
Observed that the Android usage of google meet is less when compared to iOS usage of google meet.

Results for Task 5: Point Estimates
Point estimates are used to estimate the mean of total call minutes of all days. Total Call Minutes data follows exponential distribution.
Used Central Limit Theorem by taking the distribution of many sample means. Calculated confidence intervals and z-critical values.
Point estimate based on a sample of 400 observations overestimated the true call minutes mean by 22.560137 minutes.
The distribution has high skewness, and the plot revealed the data is clearly not normal. The sample drawn from this data is also having same shape.
Created a sampling distribution by taking 300 samples from our call minutes data and then making 300 point estimates of the mean.
The sampling distribution appeared to be normal after applying Central Limit Theorem.
In addition, the mean of the sampling distribution approached the true mean, there is only a difference of 3 minutes.
Calculated 95% confidence for the mean point estimate and noticed that the calculated confidence interval is very close the true call minutes mean of 1148.051565
24 out of 25 samples of the 95% confidence intervals overlapped the red line marking the true mean.

Results for Task 6: Hypothesis testing
Tested whether the mean of total call minutes for Google Meet in the year 2019 differs from the year 2018 
Tested whether the Mean of number of meetings in google meet is similar to the Mean of number of meetings in google meet using iOS or Android.
Used Two-Sample T-test as comparison is between means of two independent data samples for all the scenarios
In all the cases, Null hypothesis is that the means of both groups are the same. Alternate Hypothesis is that the mean is different.
For all scenarios, p value is less than 0.05 which means that we reject the null hypothesis. 
Observation is that the Mean of total Call Minutes of Google Meet in year 2019 is different from year 2018 and 
The Mean of number of meetings in google meet is different from Mean of number of meetings in google meet using iOS as well as Android.

Results for Task 7: Machine Learning
Our mentor is interested in finding the unexpected spikes, drops, trend changes and level shifts.
Time series forecasting is a technique for the prediction of events through a sequence of time.
The techniques predict future events by analyzing the trends of the past, on the assumption that future trends will hold similar to historical trends.
Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.
Prophet is open source software released by Facebook's Core Data Science team. Facebook prophet is used to perform time series analysis on google meet metrics. 

Describe outcomes.
What were the outcomes and What are their implications?
Time series forecasting:
The input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column and the y column represents the values of a metric we wish to forecast.
We fit the model by instantiating a new Prophet object. Predictions are made for the future period of 365 days(1 Year). 
The predict method is used which will assign each row in future a predicted value. Plotted the forecast using Prophet.plot method.
Prophet.plot_components method is used to see the trends and weekly seasonality of the time series. 
Cross validations are done to assess prediction performance on a horizon of 70 days, starting with 210 days of training data in the first cutoff and then making predictions every 15 days.
Performance metrics is used to compute some useful statistics of the prediction performance. The statistics computed are mean squared error (MSE), 
root mean squared error (RMSE), mean absolute error (MAE), mean absolute percent error (MAPE), and coverage of the yhat_lower and yhat_upper estimates.

The mean absolute percent error (MAPE) expresses accuracy as a percentage of the error. Because the MAPE is a percentage, 
it can be easier to understand than the other accuracy measure statistics.
During my Initial analysis, MAPE value is not displayed as most of the actual metric values in Google Meet data are zero. 
Because MAPE divides the absolute error by the actual data, values close to 0 can greatly inflate the MAPE.
I have observed that the metric values are zero from Jan 2018 to May 2018 for most of the Google Meet metrics. My mentor suggested me that the reason is 
likely that metric didn't exist then, or google discontinued temporarily. He suggested me to perform time series analysis by ignoring them.
The predicted trends for the nex year is similar to the trends in the previous years. 
Even though after removing them, MAPE value is greater than 20 percent. Here,  MAPE does not give good results as we have a low volume of data. 
In Google Meet metrics, all the actual values are very small.

Below is the github link for all the above tasks
https://github.com/UNCG-CSE/G_Suite_Metrics/tree/master/src/Anusha


Google Gmail:(Henry Reichard):

Goals: The goals for this project is the same for the overall project: Analysis, Anomalies, Prediction
“Organizing Information Technology Services’ data to make it useful, accessible, actionable and aligned with data-informed decision making ideals.”

Tasks: Data Cleaning, Data Statistics and Analysis, Machine Learning (Time Series Analysis). The results will be listed below.

Data Cleaning: 
Data itself was relatively clean for Gmail, there was only missing days or missing data. These were not significant, around
60 days total of missing days or data. I originally dropped them but I eventually filled them with predicted data.

Data Statisticss:
Attributes I focused on for stats: 
num_emails_received
num_emails_sent
num_inbound_rejected_emails
num_inbound_spam_emails
num_inbound_non_spam_emails
Task 1: Extraction of Google Email data and  data cleaning
Task 2: Statistical Evaluation
Task 3: Distribution Modeling
Task 4: Point Estimates
Task 5: Finding Correlation and Co-variance within the data
Task 6: Hypothesis testing
Below is the github link for all the above tasks
https://github.com/UNCG-CSE/G_Suite_Metrics/tree/master/src/Henry

Results for Task 1:
the data is collected from 03-23-2015 to 08-19-2019. I have separated the gmail related data from the original dataset, and created a new data frame and csv file so 
I can preform data analysis on my service.I cleaned the data by dropping nan value rows and dropping 0 value rows that occurs when G-Suites are down for maintenence. Luckily the number of dropped rows
were marginal so it did not effect the data that much. Also the data is relatively clean so I didn't have to do anymore data cleaning. 
I also move all unique attributes from the rows to be there own columns, so now I have a dataframe with 28 columns and 1545 rows. Each row represents a new day from
3-23-2015 through 08-19-2019. I set the row column as index and passed it through pd.datetime. Now it is easy for me to plot and index the data.

Results for Task 2: 
Doing this task is relatively easy, df.describe will give you the basic statistics immediatly. I did this call for the overall gmail data and the 5 attributes my mentor wants
me to focus on.
Mean: emails received =280777, emails sent = 27640, inbound rejected emails = 16359, inbound spam emails = 44881, inbound non spam emails = 162224
and the mean for inbound non-spam emails are 162224.
Max: emails received = 648823, emails sent = 56768, inbound rejected emails = 144169, inbound spam emails = 127402, inbound nonspam emails = 442820
Min: emails received = 74919, emails sent = 1181, inbound rejected emails = 0, inbound spam emails = 9578, inbound nonspam emails = 50814
Variance: emails received = 1.323908e+10, emails sent = 2.538946e+08, inbound rejected emails = 3.410658e+08, inbound spam emails = 4.104442e+08, inbound nonspam emails = 3.070721e+09
Count: 1544 for all of them.

Results for Task 3:
The five attributes are discrete and  the distribution fit is poisson distribution because it shows how many  times an event likely to occur over specific time. 
The estimators I used is the kernel density estimator to check my distribution modelling and Maximum Likely Estimator using continous distributions to find the best fit for
the data. I ran a kstests to to find which estimator has the better fit. I also went ahead and did this for 7 more attributes.

Results for Task 4:
For the point estimates, I have taken samples from emails_received and used central limit theorem to show the samples look normal and the 
mean is much closer to the actual mean. I have calculated the confidence level for emails received and the sample means falls within the confidence level.
I have also calculated the confidence intervals for the  z- critical values on 11 other attributes as well.

Results for Task 5:
I checked the covariance and correlation of the 5 parameters: 
Emails received and emails sent correlation: .95
Emails receive and inbound non spam emails correlation: .88
Emails sent and inbound non spam emails correlation: .83
These correlations are very strong since it very close to 1, the correlations for the rest of the 5 attributes are not past .50 and thus are weak.

Results for Task 6:
I formed the hypothesis that the number of the 1day active users is smaller in December when compared to active users for the overall year.
I preformed a 1 sample t test with 1 day active users in december and mean of 1 day users throughout the year.
The p value is 1.22800432e-10, which is <0.05 which means we to reject the null hypothesis and it shows the distributions are different. 
Thus the mean values of the sample mean (dec) and the population mean (odu) are different. Since the mean for december is 14764 and the mean for odu is 17342, and the p value 
is much less then 0.025 (the probability of finding the data in the tail ends of the distrubition, 0.025 is on the left side with values smaller then the mean) it is concluded that 
month of december has less active users.
I also did hypothesis testing emails received and emails and I conducted a 2 sample t-test. pvalue returns 0 when the probability is super small, which means it much less then 0.05 
so the null hypothesis is rejected. These distributions are different. I did hypothesis testing for inbound emails and inbound spam emails and conducted a 2 sample t -test and
the p value was super small as well and the null hypothesis is rejected. Finally I did the same hypothesis test I did for 1 day active users for all 5 attributes and I gotten the same
results, which is amount of email traffic in december is smaller and significantly different compared to the rest of the year.

Machine Learning (Time Series Analysis):
Same attributes listed above, but I worked on additional attributes:
1day_active_users
num_emails_exchanged
inbound_delivered_emails
outbound_rejected_emails
Question asked:

The question that I mainly posed on my data, is can I predict the data of 2020 based on my current available data and
how accurate would be the predictions and if I can fill 0s or missing days in my dataset.

Methods Used: 
The methods used in answering this question is Time series analysis using fb prophet.

What was done to solve the question: 
Written a function that sent the time column and the attribute to the fb prophet function. The fb prophet function returns a crossvalidation head and tail, and a plot of the prediction and
the datapoints. Additionally written a function to send the data and return a MAPE graph to visualize the error. Some predictions gave negative values and after talking with the mentor and 
with Dr.Mohanty I went ahead and cap the negative values with 0. I filled 0s and missing days in the dataset with the mentor picked attributes with predicted values

Outcomes:
For emails_Received, emails_sent, 1day_active_users,email_exchange got a really good predictions (MAPE score close to 6% to 23%) for 365 days out into the future. For inbound_non_spam_emails 
and outbound_non_spam_emails it has a pretty decent MAPE score of around 19% for 40 days into the future prediction and around 35% and 38% at the end of the year prediction. It can give a very 
good prediction around 50 days in advance, which isnt too bad.For inbound_rejected_emails and inbound_spam_emails they have MAPE score of around 48% and 30% at 40 days into the future and 240% 
and 84% at 365 days. Inbound_rejected_emails have some anomolies in the data because there is strong spikes around the first half of the data and a period of growth and regression before it 
drops off around midpoint of 2017. After the drop off the growth is stagnated and no apparent spikes. This could explain the poor MAPE score because the data is vastly different in the first 
half and the second half. The MAPE score has improved once the time series analysis was performed on the second half of the data only. Inbound_spam_emails is preculiar as well because there is a 
massive drop off around the midpoint of 2018 and the growth is sluggish afterwards. This could explain the poor MAPE score near the end of the prediction as well. Also to note, fbprophet gives a 
downward trend for almost all of the attributes except for 1day_active_users. This implies the amount of users is increasing but the utilization of these attributes are decreasing.

Their Implications:
The decent predictions for emails_Received, emails_sent, 1day_active_users, email_exchange, inbound_non_spam_emails, inbound_delivered_emails will give the ITS department a firm understanding what 
is happening with Gmail throughout the year and allow the ITS department to prepare in advance to seasonal and overall trends in Gmail. The big question is why these services are not growing 
proportional to the growth of UNCG? After discussing with the mentor possible conclusions has been made why this is happening. These possible conclusions are the following:
 1) Students don't send emails as much as they used to 5 years ago. 
 2) People are using chat more than email now. 
 3) Google is doing a better job of blocking spam from being received in the first place.
 4) Active users however are expected to see similar growth in relation to the overall population. It's possible students/users aren't using their accounts as much as they used to. 
 5) Students who graduated or Faculty leaving UNCG might not be the same "quality" users as users coming into UNCG, as in a new users are not as in grained as older users and will not 
use it that much in the beginning. With a influx of new users over the years it could explain why the growth is not proportional. The anomolies found in inbound_rejected_emails and inbound_spam_emails 
can be explained by multiple of reasons. Before Nick (The mentor) joined the ITS Department, the ITS department used to be only reactive too attacks or spam. Over the years the ITS department took the 
"best defense is the best offense" approach by being more proactive. Additionally it is likely Google is doing a better job of handling rejections than they were before. IE : better at spam and phishing 
prevention. These reasons explains why there is a huge drop off in these attributes. The implication here is that there is progress being made to limit spam or hazardous emails and that is a good thing. 

Dashboard Development and Google Calendar Analysis (Jackie Cuong):

Goals: To analyze the google calendar data set, visualize the data set, make predictions on the data set, and develop a dashboard for the G_Suite metrics

Tasks-Data Statistics and Analysis, Machine Learning, Dashboard development

Results obtained
1. DATA STATISTICS/ANALYSIS- Analyzed the data using python I found almost no anomalies with the data set other than 1 specific date in May which had a metric of 10,000 compared to the average of the rest of the data set at 3,500, since this was such a huge anomaly I decided to drop it.  I also found that despite a growing UNCG student body, the number of daily users for the calendar dataset does not change much if at all.  I also tested a hypothesis that during the third part of the week, google calendar would see more use, but the results prove that wasn't the case as the P-Value that was returned was less than .025
2. MACHINE LEARNING-For machine learning the data acted as predicted and did not have any strange readings, with the predicted data set predicting a very slight increase in google calendar daily use at the end of 2020
3.DASHBOARD- For the dashboard I added time series graphs for every dataset and also for the predicted data set csv.  For each data set there is also a data filter for the ability to change which metric to analyze and also a date filter to change which dates the graph displays.


Google Account and Dashboard Development (Hieu Vo):

Goals:
The goals is to do analysis on the data that was given to us related to G Suite by the UNCG ITS, and find the predictions and the anomalies in the data.
As part of this I have chosen the google account service where, I have performed statistical analysis, hypothesis testing, pandas_ profiling and machine learnings.

So, the metrics that are mainly focused on in the drive data are:
google.accounts:num_7day_logins                  
google.accounts:num_1day_logins                           
google.accounts:num_suspended_users                  
google.accounts:num_disabled_accounts 
google.accounts:num_30day_logins               
google.accounts:num_users                                                                    
google.accounts:gplus_photos_used_quota_in_mb                  
google.accounts:gmail_used_quota_in_mb                         
google.accounts:total_quota_in_mb                              

Tasks:
Task 1: Extraction of Google account data
Task 2: Statistical evaluation of data
Task 3: Finding Correlation and Co-variance for different metrics
Task 4: Point Estimates
Task 5: Hypothesis testing
Task 6: Machine Learning
Task 7: Dash Board

Results obtained

Task 1 Results: Extraction of Google account data:
I have extracted the account data from the original data set and divide them into different csv file. I will later use theses csv files for pandas profiling and difference methods I will use such as machine learning. I also transform data into different form because different methods require different specific input.

Task 2: Statistical evaluation
I have performed statistical evaluation using pandas.describe(). This method helped me to obtain max, min, count, mean and other useful information from each metric in google account. I also create a method create line and bar graph for each month for each metric. Pandas profiling also show missing value and bar graph for each metric.

Task 3 Results: Finding Correlation and Covariance for different metrics
The correlation and covariance between google.accounts:num_1day_logins and google.accounts:num_30day_logins is 1002630.41 and 0.29. The correlation score is small and closer to 0 so 2 metrics are not highly correlated. We can predict that there is no significant relationship between the 2 metrics. Pandas profiling also provided a correlation matrix.

Task 4 Results: Point Estimates
Point estimates are used to estimate the mean value of a metric. Calculated the the sample means of the metric by taking the sample size of all google.accounts:num_1day_logins.
I plotted the sample means which follows a normal distribution using the Central limit theorem. I was able to find the z-critical value: 1.95996398454005 and the Confidence Interval: (7221.476370852023, 7657.689629147976) and t-critical value: 2.0638985616280205 and Confidence interval: (5588.076965295933, 8788.963034704067)

Task 5 Results: Hypothesis testing
The null hypothesis is that the means of both groups, google.accounts:drive_used_quota_in_mb and google.accounts:gmail_used_quota_in_mb, are the same.
I did the two sample t-test to the 2 metrics. The p value is 2.5775440597091544e-55, which is < 0.05 means we accept the rejection of the null hypothesis. 

Task 6 Results: Machine Learning.
Functions from fb prophet are used to predict and forecast the data of 2020 based on the data from previous years. The method predicted with good accuracy. Mse(Mean squared error), mape(Mean absolute percentage error) as measures for error and accuracies show that the error is low. The regression model show good results as well when the graph of the data shows a decent straight line and the result graph also shows a straight line.

Task 7: Dash Board.
The dash board helps to present data better and easy to understand. We mainly use line graph and pie graph to show the trend of the data.

Outcomes:
Their Implications: Statistical evaluation and pandas profiling gave a detailed overall quality of the data set such as missing value, occerenace and warnings about bad data. Monthly line graph help to recognize anomalies and trends in smaller scale.
Correlation analysis and correlation matrix help to investigate the relationship between metrics.
Hypothesis testing allows us to draw conclusions about an entire population based on a representative metrics.
Estimator help to prevent biased sampling methods and the randomness inherent to drawing a sample from a population and help us to find out if the sample mean is the same as the population mean.
FB prophet help to predict the future trend of each metric. Linear regression model help to predict storage used for a different amount of user.
The dash board helps to present data better and easy to understand.


Below is the github link for all my tasks for the project
https://github.com/UNCG-CSE/G_Suite_Metrics/tree/master/src/Hieu



